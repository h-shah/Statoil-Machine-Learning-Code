{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn import linear_model,svm, metrics\n",
    "import winsound\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data from pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = 'D:/Libraries/Documents/Tensorflow/tensorflow/Statoil/'\n",
    "with open(folder+'labeled_data.pkl','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "b1train= data['b1train']\n",
    "b1valid= data['b1valid']\n",
    "b1test= data['b1test']\n",
    "b2train= data['b2train']\n",
    "b2valid= data['b2valid']\n",
    "b2test= data['b2test']\n",
    "anglestrain= data['anglestrain']\n",
    "anglesvalid= data['anglesvalid']\n",
    "anglestest= data['anglestest']\n",
    "labelstrain= data['labelstrain']\n",
    "labelsvalid= data['labelsvalid']\n",
    "labelstest= data['labelstest']\n",
    "idstrain= data['idstrain']\n",
    "idsvalid= data['idsvalid']\n",
    "idstest= data['idstest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try some 'off the shelf' sklearn classifiers. Logistic Regression, SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((b1train, b2train), axis=1)\n",
    "Xvalid = np.concatenate((b1valid,b2valid),axis=1)\n",
    "Xtest = np.concatenate((b1test,b2test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = linear_model.LogisticRegression()\n",
    "LR.fit(X=X,y=labelstrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Score: 0.74\n",
      "Log Loss: 0.763588456889\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw Score: \" + str(LR.score(Xvalid,labelsvalid)))\n",
    "pred_proba = LR.predict_proba(Xvalid)\n",
    "print(\"Log Loss: \" + str(metrics.log_loss(labelsvalid,pred_proba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Gives:\n",
    "Raw Score: 0.74\n",
    "Log Loss: 0.763588456889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernels = ['rbf', 'linear', 'sigmoid']\n",
    "for k in kernels:\n",
    "    SVC = svm.SVC(kernel=k)\n",
    "    SVC.fit(X=X,y=labelstrain)\n",
    "    print(\"kernel = \" + str(k))\n",
    "    print(\"Raw Score: \" + str(SVC.score(Xvalid,labelsvalid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernel = rbf\n",
    "Raw Score: 0.52\n",
    "kernel = linear\n",
    "Raw Score: 0.73\n",
    "kernel = sigmoid\n",
    "Raw Score: 0.52\n",
    "    \n",
    "Seems like the data is fairly linearly separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xnorm = np.square(b1train)+np.square(b2train)\n",
    "Xnormvalid = np.square(b1valid)+np.square(b2valid)\n",
    "Xnormtest = np.square(b1test)+np.square(b2test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR = linear_model.LogisticRegression()\n",
    "LR.fit(Xnorm,labelstrain)\n",
    "\n",
    "print(\"Raw Score: \" + str(LR.score(Xnormvalid,labelsvalid)))\n",
    "pred_proba = LR.predict_proba(Xnormvalid)\n",
    "print(\"Log Loss: \" + str(metrics.log_loss(labelsvalid,pred_proba)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernels = ['rbf', 'linear', 'sigmoid']\n",
    "for k in kernels:\n",
    "    SVC = svm.SVC(kernel=k)\n",
    "    SVC.fit(X=Xnorm,y=labelstrain)\n",
    "    print(\"kernel = \" + str(k))\n",
    "    print(\"Raw Score: \" + str(SVC.score(Xnormvalid,labelsvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xangles = np.arctan2(b2train,b1train)\n",
    "Xanglesvalid = np.arctan2(b1valid,b2valid)\n",
    "Xanglestest = np.arctan2(b1test,b2test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR = linear_model.LogisticRegression()\n",
    "LR.fit(Xangles,labelstrain)\n",
    "\n",
    "print(\"Raw Score: \" + str(LR.score(Xanglesvalid,labelsvalid)))\n",
    "pred_proba = LR.predict_proba(Xanglesvalid)\n",
    "print(\"Log Loss: \" + str(metrics.log_loss(labelsvalid,pred_proba)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpolar = np.concatenate((Xnorm,Xangles),axis=1)\n",
    "Xpolarvalid = np.concatenate((Xnormvalid,Xanglesvalid),axis=1)\n",
    "Xpolartest = np.concatenate((Xnormtest, Xanglestest),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR = linear_model.LogisticRegression()\n",
    "LR.fit(Xpolar,labelstrain)\n",
    "\n",
    "print(\"Raw Score: \" + str(LR.score(Xpolarvalid,labelsvalid)))\n",
    "pred_proba = LR.predict_proba(Xpolarvalid)\n",
    "print(\"Log Loss: \" + str(metrics.log_loss(labelsvalid,pred_proba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try a Neural Network. Start with 3 hidden layers and relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we convert the labels to one-hot encodings for the tensorflow cross-entropy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 1]\n",
      "[[ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(labelstest[0:5])\n",
    "onehot_test = np.ndarray(shape=[labelstest.shape[0],2])\n",
    "onehot_test[:,0]=(labelstest == 0)\n",
    "onehot_test[:,1]=(labelstest == 1)\n",
    "print(onehot_test[0:5,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 0]\n",
      "[[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(labelsvalid[0:5])\n",
    "onehot_valid = np.ndarray(shape=[labelsvalid.shape[0],2])\n",
    "onehot_valid[:,0]=(labelsvalid == 0)\n",
    "onehot_valid[:,1]=(labelsvalid == 1)\n",
    "print(onehot_valid[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0]\n",
      "[[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(labelstrain[0:5])\n",
    "onehot_train=np.ndarray(shape=[labelstrain.shape[0],2])\n",
    "onehot_train[:,0]=(labelstrain == 0)\n",
    "onehot_train[:,1]=(labelstrain == 1)\n",
    "print(onehot_train[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the list of Xs so that we can iterate over our various representations of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parameters\n",
    "num_hidden = 1024\n",
    "learning_rate=0.0002\n",
    "dropout_rate=0.3\n",
    "num_labels = 2\n",
    "\n",
    "Xs = [X]#, Xpolar]\n",
    "Xvalids = [Xvalid]#, Xpolarvalid]\n",
    "Xtests = [Xtest]#, Xpolartest]\n",
    "\n",
    "for X,Xv,Xt in zip(Xs,Xvalids,Xtests):\n",
    "    num_features = X.shape[1]\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        \n",
    "        #input training data\n",
    "        tf_X = tf.placeholder(dtype=tf.float32, shape=(None, num_features))\n",
    "        tf_y = tf.placeholder(dtype=tf.float32, shape=(None, num_labels))\n",
    "        tf_is_training = tf.placeholder(dtype=tf.bool)\n",
    "        \n",
    "        #hidden layers\n",
    "        tf_hidden_1 = tf.layers.dense(tf_X,num_hidden)\n",
    "        tf_hidden_1 = tf.nn.relu(tf_hidden_1)\n",
    "        tf_hidden_1 = tf.layers.dropout(tf_hidden_1, rate = dropout_rate, training=tf_is_training)\n",
    "        \n",
    "        tf_hidden_2 = tf.layers.dense(tf_hidden_1,num_hidden)\n",
    "        tf_hidden_2 = tf.nn.relu(tf_hidden_2)\n",
    "        tf_hidden_2 = tf.layers.dropout(tf_hidden_2, rate = dropout_rate, training=tf_is_training)\n",
    "        \n",
    "        tf_hidden_3 = tf.layers.dense(tf_hidden_2,num_hidden)\n",
    "        tf_hidden_3 = tf.nn.relu(tf_hidden_3)\n",
    "        #tf_hidden_3 = tf.layers.dropout(tf_hidden_3, rate=dropout_rate, training=tf_is_training)\n",
    "        \n",
    "        #output\n",
    "        tf_output = tf.layers.dense(tf_hidden_3,num_labels)\n",
    "        tf_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_y,logits=tf_output))#Need to change to log loss\n",
    "        tf_optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(tf_loss)\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Step:  0\n",
      "Loss:  9.24991\n",
      "Validation Accuracy:  0.52\n",
      "Validation Loss:  39.7753\n",
      "Step:  500\n",
      "Loss:  0.624504\n",
      "Validation Accuracy:  0.57\n",
      "Validation Loss:  0.60793\n",
      "Step:  1000\n",
      "Loss:  0.650186\n",
      "Validation Accuracy:  0.7\n",
      "Validation Loss:  0.588915\n",
      "Step:  1500\n",
      "Loss:  0.618067\n",
      "Validation Accuracy:  0.66\n",
      "Validation Loss:  0.592986\n",
      "Step:  2000\n",
      "Loss:  0.508514\n",
      "Validation Accuracy:  0.64\n",
      "Validation Loss:  0.625163\n",
      "Step:  2500\n",
      "Loss:  0.465766\n",
      "Validation Accuracy:  0.73\n",
      "Validation Loss:  0.529888\n",
      "Step:  3000\n",
      "Loss:  0.567861\n",
      "Validation Accuracy:  0.74\n",
      "Validation Loss:  0.513416\n",
      "Step:  3500\n",
      "Loss:  0.488856\n",
      "Validation Accuracy:  0.72\n",
      "Validation Loss:  0.528653\n",
      "Step:  4000\n",
      "Loss:  0.571767\n",
      "Validation Accuracy:  0.7\n",
      "Validation Loss:  0.531931\n",
      "Step:  4500\n",
      "Loss:  0.420224\n",
      "Validation Accuracy:  0.74\n",
      "Validation Loss:  0.496103\n",
      "Step:  5000\n",
      "Loss:  0.522784\n",
      "Validation Accuracy:  0.72\n",
      "Validation Loss:  0.509659\n",
      "Step:  5500\n",
      "Loss:  0.407494\n",
      "Validation Accuracy:  0.75\n",
      "Validation Loss:  0.494307\n",
      "Step:  6000\n",
      "Loss:  0.531231\n",
      "Validation Accuracy:  0.55\n",
      "Validation Loss:  0.706402\n",
      "Step:  6500\n",
      "Loss:  0.462687\n",
      "Validation Accuracy:  0.71\n",
      "Validation Loss:  0.522072\n",
      "Step:  7000\n",
      "Loss:  0.396498\n",
      "Validation Accuracy:  0.75\n",
      "Validation Loss:  0.473659\n",
      "Step:  7500\n",
      "Loss:  0.306258\n",
      "Validation Accuracy:  0.64\n",
      "Validation Loss:  0.619151\n",
      "Step:  8000\n",
      "Loss:  0.386181\n",
      "Validation Accuracy:  0.76\n",
      "Validation Loss:  0.477953\n",
      "Step:  8500\n",
      "Loss:  0.470277\n",
      "Validation Accuracy:  0.78\n",
      "Validation Loss:  0.430406\n",
      "Step:  9000\n",
      "Loss:  0.298957\n",
      "Validation Accuracy:  0.71\n",
      "Validation Loss:  0.477436\n",
      "Step:  9500\n",
      "Loss:  0.509563\n",
      "Validation Accuracy:  0.71\n",
      "Validation Loss:  0.507978\n",
      "Step:  10000\n",
      "Loss:  0.381055\n",
      "Validation Accuracy:  0.82\n",
      "Validation Loss:  0.445282\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_steps=10001\n",
    "  \n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (X.shape[0] - batch_size)\n",
    "        batch_X = X[offset:(offset + batch_size), :]\n",
    "        batch_labels = onehot_train[offset:(offset + batch_size),:]\n",
    "        feed_dict = {tf_X: batch_X, tf_y: batch_labels, tf_is_training: True}\n",
    "        \n",
    "        _, loss = session.run([tf_optimizer, tf_loss], feed_dict=feed_dict)\n",
    "        \n",
    "        if(step % 500==0):\n",
    "          print(\"Step: \", step)\n",
    "          print(\"Loss: \", loss)\n",
    "          valid_preds,valid_loss = session.run([tf_output,tf_loss], feed_dict={tf_X:Xvalid,tf_y: onehot_valid,tf_is_training:False})\n",
    "          print(\"Validation Accuracy: \", np.mean(np.argmax(valid_preds,axis=1)==labelsvalid))\n",
    "          print(\"Validation Loss: \", valid_loss)\n",
    "\n",
    "winsound.Beep(500,1000)\n",
    "          \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad accuracy. Still not competitive. Could probably benefit from decaying learning rate, but it's difficult to train well with such limited training examples. Probably a convolutional network will do much better since more specific models are more practical when we have a small amount of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
